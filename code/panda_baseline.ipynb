{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Image augmentation\n",
    "from albumentations import Compose, Normalize, HorizontalFlip, VerticalFlip\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from skimage import io\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this cell if run locally\n",
    "# This takes the pretrained model into a location pytorch checks when we call torch.hub.load(...)\n",
    "!mkdir 'cache'\n",
    "!mkdir 'cache/torch'\n",
    "!mkdir 'cache/torch/checkpoints'\n",
    "!cp '../input/pytorch-pretrained-models/semi_supervised_resnext50_32x4-ddb3e555.pth' 'cache/torch/checkpoints/'\n",
    "torch.hub.DEFAULT_CACHE_DIR = '/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.DEFAULT_CACHE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>This way we can call models and model inputs <>.to(device) and have it work regardless if on cpu or gpu</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_memory():\n",
    "    import collections, gc, resource, torch\n",
    "    print('maxrss = {}'.format(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "    tensors = collections.Counter((str(o.device), o.dtype, tuple(o.shape))\n",
    "                                  for o in gc.get_objects()\n",
    "                                  if torch.is_tensor(o))\n",
    "    for line in tensors.items():\n",
    "        print('{}\\t{}'.format(*line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Seen a lot of people on Kaggle set all seeds in one place </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "batch_size = 16\n",
    "N = 10  # number of tiles per image\n",
    "\n",
    "# Need to change if putting onto a Kaggle kernel?\n",
    "TILES = '/kaggle/input/panda-16x128x128-tiles-data/train/'\n",
    "LABELS = '/kaggle/input/prostate-cancer-grade-assessment/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(LABELS).set_index('image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> Should check at some point if test set has similar distribution of target labels. Thought I saw in paper that test set was more heavily biased towards Grade 5 images...</>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isup_grade.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only consider images we have processed and stored as tiles in TRAIN folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = {filepath[:32] for filepath in os.listdir(TILES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[image_ids]\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lose about 100 images (from IAFoss pre-processing, he only used images with masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(data, test_size=0.3, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True, drop=True)\n",
    "valid.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TileTrainDataSet(Dataset):\n",
    "    def __init__(self, df, transform_fn=None):\n",
    "        self.X = df['image_id']\n",
    "        self.Y = df['isup_grade']\n",
    "        self.transform = transform_fn\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Take image id and use the first N tiles (all have the same target label)\n",
    "        img_id = self.X[idx]\n",
    "        imgs = []\n",
    "        for i in range(N):\n",
    "            img = io.imread(os.path.join(TILES,img_id+f\"_{i}.png\"))\n",
    "            \n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=img)\n",
    "                img = augmented['image']\n",
    "            imgs.append(img)\n",
    "        # Final shape is x:  N x 3 x 128 x 128, y: 1\n",
    "        x = torch.stack(imgs)\n",
    "        return x, self.Y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transforms(*, partition):\n",
    "    \n",
    "    assert partition in ('train', 'valid')\n",
    "    \n",
    "    if partition == 'train':\n",
    "        return Compose([\n",
    "            HorizontalFlip(p=0.5),  # 50/50 chance of performing horizontal flip\n",
    "            VerticalFlip(p=0.5),\n",
    "            # Normalize images according to ResNext specifications\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif partition == 'valid':\n",
    "        # Don't flip validation data \n",
    "        return Compose([\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TileTrainDataSet(train, transform_fn=img_transforms(partition='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a set of tiles we have just one label (as the tiles will be concatenated in later layers of the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tile is a 128x128 rgb (3-channel) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy is to treat each tile as independent. Pass it into the network individually (not as a single composite image of tiles). <br>So we reshape from batch_size x N to (batch_size*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].view(-1, shape[2], shape[3], shape[4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = TileTrainDataSet(valid, transform_fn=img_transforms(partition='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # n=6 represents number of label classes, give better name. \n",
    "    # Except for now doing regression instead of classification\n",
    "    def __init__(self, arch='resnext50_32x4d_ssl', n=6, pre=True):\n",
    "        super().__init__()\n",
    "        m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', arch)\n",
    "        self.enc = nn.Sequential(*list(m.children())[:-2])  # Remove last two layers from ResNext\n",
    "        nc = list(m.children())[-1].in_features  # 2048 (last linear layer of resnext50)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear1 = nn.Linear(nc,512)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear2 = nn.Linear(512,1)\n",
    "                                 \n",
    "    def forward(self, x):\n",
    "        # Original shape: bs x N x 3 x 128 x 128\n",
    "        shape = x.shape\n",
    "        x = x.view(-1,shape[2],shape[3],shape[4])  # bs*N x 3 x 128 x 128\n",
    "        # C represents output_size from ResNext\n",
    "        x = self.enc(x)  # bs*N x C x 4 x 4\n",
    "        \n",
    "        shape = x.shape\n",
    "        # concatenate the output for tiles into a single map\n",
    "        # Need to do in two steps to 1) Separate batch_size and N, 2) Combine N into outer dimensions \n",
    "        # Result: bs x C x N*4 x 4\n",
    "        x = x.view(-1,N,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n",
    "          .view(-1,shape[1],shape[2]*N,shape[3])  \n",
    "        \n",
    "        # With 2-D pooling over size 1, reduces last two dimensions to 1 \n",
    "        x = self.pool(x)  # bs x C x 1 x 1\n",
    "        # Flatten last three dimensions (result: bs x C)\n",
    "        x = self.linear1(torch.flatten(x, start_dim=1))  # bs x 512\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        # Look at other pre-trained models intended for regression?\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_dl, epochs,):\n",
    "    iterations = epochs*len(train_dl)\n",
    "    pbar = tqdm_notebook(total=iterations)\n",
    "    best_kappa = 0.0\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "\n",
    "        for img, label in train_dl:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device).float().unsqueeze(1)\n",
    "            out = model(img)\n",
    "            # some suggest since kappa is a quasi-measure of \"distance\" from true label, \n",
    "            # better to calculate MSE regression loss than classification loss\n",
    "            loss = F.mse_loss(out, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += label.size(0)*loss.item()\n",
    "            total += label.size(0)\n",
    "            pbar.update()\n",
    "            \n",
    "        train_loss = total_loss/total\n",
    "        \n",
    "        val_loss, val_kappa = valid_metrics(model, valid_dl)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"\\tTrain loss: {train_loss:.3f} \\t Valid loss: {val_loss:.3f} \\t Valid Kapp:  {val_kappa:.3f}\")\n",
    "        \n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            path = f\"/kaggle/working/best_model.pth\"\n",
    "            save_model(model, path)\n",
    "\n",
    "            print(f\"Best kappa: {best_kappa:.3f}\")\n",
    "    return best_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for img, label in valid_dl:\n",
    "        img = img.to(device)\n",
    "        batch = label.shape[0]\n",
    "        out = model(img)\n",
    "        loss = F.mse_loss(out, label.to(device).float().unsqueeze(1))\n",
    "        total_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        \n",
    "        preds.append(out.detach().to('cpu').apply_(threshold).long().numpy())\n",
    "        labels.append(label.long().unsqueeze(1).numpy())\n",
    "    \n",
    "    preds, labels = np.vstack(preds), np.vstack(labels)\n",
    "    val_loss = total_loss/total\n",
    "    val_kappa = cohen_kappa_score(preds, labels)\n",
    "    return val_loss, val_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(x):\n",
    "    \"\"\"\n",
    "    Our model has a regression loss function, \n",
    "    but we need to convert those values to the nearest classification label value (0,1,2,3,4,5) \n",
    "    \"\"\"\n",
    "    return max(\n",
    "                min(round(x),5)\n",
    "            ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model, '/kaggle/working/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, scheduler, train_dl, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
